{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MLP (Multi Layer Perceptron)\n",
    "\n",
    "This is a model which there are perceptrons chained one another. Pn input is the Pn-1 output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "This is the MLP Sigmoid activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f <- function(net) {\n",
    "\tret = 1.0 / (1.0 + exp(-net))\n",
    "\treturn (ret)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the MLP Architecture\n",
    "\n",
    "This function is used to build up the MLP architecture, i.e., the neurons contained in the hidden and the output layers with their respective weights and thetas randomically initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp.architecture <- function(input.layer.size = 2,\n",
    "\t\thidden.layer.size = 2,\n",
    "\t\toutput.layer.size = 1,\n",
    "\t\tf.net = f) {\n",
    "\n",
    "\t# Here we create a list to contain the layers information\n",
    "\tlayers = list()\n",
    "\n",
    "\t# This is the hidden layer in which weights and thetas\n",
    "\t# were initialized in a random manner (using runif) in\n",
    "\t# interval [-1,1]. Term input.layer.size+1 refers to\n",
    "\t# the number of neurons in the input layer (a weight per unit),\n",
    "\t# plus an additional element to define theta\n",
    "\tlayers$hidden = matrix(runif(min=-1, max=1, \n",
    "\t\t\t\t  n=hidden.layer.size*(input.layer.size+1)), \n",
    "\t\t\t       nrow=hidden.layer.size, \n",
    "\t\t\t       ncol=input.layer.size+1)\n",
    "\n",
    "\t# The same as the hidden layer happens here, but for the output layer\n",
    "\tlayers$output = matrix(runif(min=-1, max=1, \n",
    "\t\t\t\t  n=output.layer.size*(hidden.layer.size+1)), \n",
    "\t\t\t       nrow=output.layer.size, \n",
    "\t\t\t       ncol=hidden.layer.size+1)\n",
    "\n",
    "\t# Defining a list to return everything:\n",
    "\t# - the number of units or neurons at the input layer\n",
    "\t# - the number of units at the hidden layer\n",
    "\t# - the number of units at the output layer\n",
    "\t# - layers information (including weights and thetas)\n",
    "\t# - the activation function used is also returned\n",
    "\tret = list()\n",
    "\tret$input.layer.size = input.layer.size\n",
    "\tret$hidden.layer.size = hidden.layer.size\n",
    "\tret$output.layer.size = output.layer.size\n",
    "\tret$layers = layers\n",
    "\tret$f.net = f.net\n",
    "\n",
    "\treturn (ret)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward function\n",
    "\n",
    "This function produces the MLP output after providing input values. Term architecture refers to the model produced by function mlp.architecture. Term dataset corresponds to the examples used as input to the MLP. Term p is associated to the identifier of the current example being forwarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forward <- function(architecture, dataset, p) {\n",
    "\n",
    "\t# Orgazining the dataset as input examples x\n",
    "\tx = matrix(dataset[,1:architecture$input.layer.size], ncol=architecture$input.layer.size)\n",
    "\t# Orgazining the dataset as expected classes y associated to input examples x\n",
    "\ty = matrix(dataset[,(architecture$input.layer.size+1):ncol(dataset)], nrow=nrow(x))\n",
    "\n",
    "\t# Submitting the p-th input example to the hidden layer\n",
    "\tnet_h = architecture$layers$hidden %*% c(as.vector(ts(x[p,])), 1)\n",
    "\tf_net_h = architecture$f.net(net_h)\n",
    "\n",
    "\t# Submitting the hidden layer outputs as inputs for the output layer\n",
    "\tnet_o = architecture$layers$output %*% c(f_net_h, 1)\n",
    "\tf_net_o = architecture$f.net(net_o)\n",
    "\n",
    "\t# Here we have the final results produced by the MLP\n",
    "\tret = list()\n",
    "\tret$f_net_h = f_net_h\n",
    "\tret$f_net_o = f_net_o\n",
    "\n",
    "\treturn (ret)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function (Backpropagation)\n",
    "\n",
    "This function is responsible for training, i.e., adapting weights and thetas for every neuron (or unit) at the hidden and the output layer. It basically applies the Gradient Descent Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backpropagation <- function(architecture, dataset, eta=0.1, threshold=1e-3) {\n",
    "\n",
    "\tx = matrix(dataset[,1:architecture$input.layer.size], ncol=architecture$input.layer.size)\n",
    "\ty = matrix(dataset[,(architecture$input.layer.size+1):ncol(dataset)], nrow=nrow(x))\n",
    "\n",
    "\tcat(\"Input data...\\n\")\n",
    "\tprint(x)\n",
    "\n",
    "\tcat(\"Expected output...\\n\")\n",
    "\tprint(y)\n",
    "\n",
    "\tcat(\"Enter to start running...\")\n",
    "\treadline()\n",
    "\n",
    "\tsquared_error = threshold * 2\n",
    "\n",
    "\t# This loop will run until the average squared error is below\n",
    "\t# some threshold value.\n",
    "\twhile (squared_error > threshold) {\n",
    "\n",
    "\t\t# Initializing the squared error to measure the loss for\n",
    "\t\t# all examples in the training set\n",
    "\t\tsquared_error = 0\n",
    "\n",
    "\t\t# For every example at index (row) p\n",
    "\t\tfor (p in 1:nrow(x)) {\n",
    "\n",
    "\t\t\t# Applying the input example at index p\n",
    "\t\t\tf = forward(architecture, dataset, p)\n",
    "\n",
    "\t\t\t# Getting results to adapt weights and thetas\n",
    "\t\t\terror = (y[p,] - f$f_net_o)\n",
    "\n",
    "\t\t\t# Computing term delta for the output layer\n",
    "\t\t\t# which simplifies next computations involved \n",
    "\t\t\t# in the Gradient Descent method\n",
    "\t\t\tdelta_o = error * f$f_net_o * (1-f$f_net_o)\n",
    "\n",
    "\t\t\t# This is the squared error used as stop criterion.\n",
    "\t\t\t# Term sum(error^2) is used because the last layer\n",
    "\t\t\t# (i.e., the output layer) may have more than a single\n",
    "\t\t\t# neuron. We also use a power of two to ensure negative\n",
    "\t\t\t# and positive values do not nullify each other.\n",
    "\t\t\tsquared_error = squared_error + sum(error^2)\n",
    "\n",
    "\t\t\t# Computing term delta for the hidden layer\n",
    "\t\t\tw_o = architecture$layers$output[,1:architecture$hidden.layer.size]\n",
    "\t\t\tdelta_h = (f$f_net_h * (1 - f$f_net_h)) * sum(as.vector(delta_o) * as.vector(w_o))\n",
    "\n",
    "\t\t\t# Adapting weights and thetas at the output layer\n",
    "\t\t\tarchitecture$layers$output = \n",
    "\t\t\t\tarchitecture$layers$output + eta * delta_o %*% c(f$f_net_h, 1)\n",
    "\n",
    "\t\t\t# Adapting weights and thetas at the hidden layer\n",
    "\t\t\tarchitecture$layers$hidden =\n",
    "\t\t\t\tarchitecture$layers$hidden + eta * delta_h %*% c(x[p,], 1)\n",
    "\t\t}\n",
    "\n",
    "\t\t# Dividing the total squared error by nrow to find the average\n",
    "\t\t# which we decided to use as stop criterion\n",
    "\t\tsquared_error = squared_error / nrow(x)\n",
    "\n",
    "\t\t# Printing the average squared error out\n",
    "\t\tcat(\"Squared error = \", squared_error, \"\\n\")\n",
    "\t}\n",
    "\t\n",
    "\t# Returning the trained architecture, which can now\n",
    "\t# be used for execution.\n",
    "\treturn (architecture)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp.test <- function(architecture, dataset, debug=T) {\n",
    "\n",
    "\t# Orgazining the dataset as input examples x\n",
    "\tx = matrix(dataset[,1:architecture$input.layer.size], ncol=architecture$input.layer.size)\n",
    "\t# Orgazining the dataset as expected classes y associated to input examples x\n",
    "\ty = matrix(dataset[,(architecture$input.layer.size+1):ncol(dataset)], nrow=nrow(x))\n",
    "\n",
    "\tcat(\"Enter to start testing...\")\n",
    "\treadline()\n",
    "\n",
    "\toutput = NULL\n",
    "\n",
    "\t# For every example at index (row) p\n",
    "\tfor (p in 1:nrow(x)) {\n",
    "\n",
    "\t\t# Applying the input example at index p\n",
    "\t\tf = forward(architecture, dataset, p)\n",
    "\n",
    "\t\t# If debug is true, show all information regarding classification\n",
    "\t\tif (debug) {\n",
    "\t\t\tcat(\"Input pattern = \", as.vector(x[p,]), \n",
    "\t\t\t    \" Expected output = \", as.vector(y[p,]), \n",
    "\t\t\t\t\" Obtained output = \", as.vector(f$f_net_o), \"\\n\")\n",
    "\t\t}\n",
    "\n",
    "\t\t# Concatenating all output values as rows in a matrix,\n",
    "\t\t# so we can check them out later.\n",
    "\t\toutput = rbind(output, as.vector(f$f_net_o))\n",
    "\t}\n",
    "\n",
    "\t# Returning results\n",
    "\treturn (output)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing a Discrete Hyperplane\n",
    "\n",
    "This function is useful to produce a discrete (either yes or no) hyperplane to shatter the input space of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discretize.hiperplane <- function(img, range = c(0.45, 0.55)) {\n",
    "\tids_negative = which(img < range[1])\n",
    "\tids_positive = which(img > range[2])\n",
    "\tids_hiperplane = which(img >= range[1] & img <= range[2])\n",
    "\n",
    "\timg[ids_negative] = 0\n",
    "\timg[ids_positive] = 1\n",
    "\timg[ids_hiperplane] = 0.5\n",
    "\n",
    "\timg\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the MLP\n",
    "This is a function to train and test the XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xor.test <- function(eta=0.1, threshold=1e-3) {\n",
    "\n",
    "\t# Loading the dataset \"xor.dat\"\n",
    "\tdataset = as.matrix(read.table(\"xor.dat\"))\n",
    "\n",
    "\t# Building up the MLP architecture with random weights and thetas.\n",
    "\t# Observe we have two units at the input layer (what is the number\n",
    "\t# of input variables), we have two units at the hidden layer (so we\n",
    "\t# will have two hyperplanes to shatter the space of examples as\n",
    "\t# expected), and we have a single unit at the output layer to provide\n",
    "\t# the answer as 0 or 1 (actually values in range [0,1])\n",
    "\tmodel = mlp.architecture(input.layer.size = 2, hidden.layer.size = 2, output.layer.size = 1, f.net = f)\n",
    "\n",
    "\t# Now we train the architecture \"model\" to build up the \"trained.model\"\n",
    "\ttrained.model = backpropagation(model, dataset,eta=eta, threshold=threshold)\n",
    "\n",
    "\t# Then we test the \"trained.model\" using the same XOR dataset.\n",
    "\t# For more complex problems, we will use unseen examples.\n",
    "\tmlp.test(trained.model, dataset)\n",
    "\n",
    "\t# Building up hyperplanes to plot\n",
    "\tx = seq(-0.1,1.1,length=100)\n",
    "\thiperplane_1 = outer(x,x, function(x,y) { cbind(x,y,1) %*% trained.model$layers$hidden[1,] } )\n",
    "\thiperplane_2 = outer(x,x, function(x,y) { cbind(x,y,1) %*% trained.model$layers$hidden[2,] } )\n",
    "\n",
    "\tcat(\"Press enter to plot both hiperplanes...\")\n",
    "\treadline()\n",
    "\n",
    "\t# Plotting the hyperplanes built at the hidden layer\n",
    "\tfilled.contour(discretize.hiperplane(hiperplane_1) + discretize.hiperplane(hiperplane_2))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Optical Character Recognition\n",
    "\n",
    "This MLP solves the Optical Character Recognition (OCR) problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ocr.test <- function(eta=0.1, threshold=1e-3) {\n",
    "\n",
    "\t# Loading the dataset\n",
    "\tdataset = as.matrix(read.table(\"ocr-asvector.dat\"))\n",
    "\n",
    "\t# Loading a test set with unseen examples\n",
    "\ttest.dataset = as.matrix(read.table(\"test-ocr-asvector.dat\"))\n",
    "\n",
    "\t# Building up the architecture with 70 units at the input layer,\n",
    "\t# 5 units (so 5 hyperplanes) at the hidden layer and 2 at the output\n",
    "\t# layer.\n",
    "\tmodel = mlp.architecture(input.layer.size = 10*7, \n",
    "\t\t\t\t hidden.layer.size = 5, \n",
    "\t\t\t\t output.layer.size = 2, f.net = f)\n",
    "\n",
    "\t# Training\n",
    "\ttrained.model = backpropagation(model, \n",
    "\t\t\t\tdataset,\n",
    "\t\t\t\teta=eta, \n",
    "\t\t\t\tthreshold=threshold)\n",
    "\n",
    "\t# Testing for unseen examples\n",
    "\tmlp.test(trained.model, test.dataset)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
