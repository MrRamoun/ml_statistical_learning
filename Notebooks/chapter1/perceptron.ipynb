{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heaviside function with a default epsilon\n",
    "g <- function(net, epsilon=0.5) {\n",
    "\tif (net > epsilon) {\n",
    "\t\treturn (1)\n",
    "\t} else {\n",
    "\t\treturn (0)\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to train the Perceptron\n",
    "# Observe eta and threshold assume default values\n",
    "perceptron.train <- function(train.table, eta=0.1, threshold=1e-2) {\n",
    "\n",
    "\t# Number of input variables\n",
    "\tnVars = ncol(train.table)-1\n",
    "\n",
    "\tcat(\"Randomizing weights and theta in range [-0.5, 0.5]...\\n\")\n",
    "\n",
    "\t# Randomizing weights\n",
    "\tweights = runif(min=-0.5, max=0.5, n=nVars)\n",
    "\n",
    "\t# Randomizing theta\n",
    "\ttheta = runif(min=-0.5, max=0.5, n=1)\n",
    "\n",
    "\t# This sum of squared errors will accumulate all errors\n",
    "\t# occuring along training iterations. When this error is\n",
    "\t# below a given threshold, learning stops.\n",
    "\tsumSquaredError = 2*threshold\n",
    "\n",
    "\t# Learning iterations\n",
    "\twhile (sumSquaredError > threshold) {\n",
    "\n",
    "\t\t# Initializing the sum of squared errors as zero\n",
    "\t\t# to start counting and later evaluate the total\n",
    "\t\t# loss for this dataset in train.table\n",
    "\t\tsumSquaredError = 0\n",
    "\n",
    "\t\t# Iterate along all rows (examples) contained in\n",
    "\t\t# train.table\n",
    "\t\tfor (i in 1:nrow(train.table)) {\n",
    "\n",
    "\t\t\t# Example x_i\n",
    "\t\t\tx_i = train.table[i, 1:nVars]\n",
    "\n",
    "\t\t\t# Expected output class\n",
    "\t\t\t# Observe the last column of this table\n",
    "\t\t\t# contains the output class\n",
    "\t\t\ty_i = train.table[i, ncol(train.table)]\n",
    "\n",
    "\t\t\t# Now the Perceptron produces the output\n",
    "\t\t\t# class using the current values for \n",
    "\t\t\t# weights and theta, then it applies the\n",
    "\t\t\t# heaviside function\n",
    "\t\t\that_y_i = g(x_i %*% weights + theta)\n",
    "\n",
    "\t\t\t# This is the error, referred to as (y_i - g(x_i))\n",
    "\t\t\t# in the Perceptron formulation\n",
    "\t\t\tError = y_i - hat_y_i\n",
    "\n",
    "\t\t\t# As part of the Gradient Descent method, we here\n",
    "\t\t\t# compute the partial derivative of the Squared Error\n",
    "\t\t\t# for the current example i in terms of weights and theta.\n",
    "\t\t\t# Observe constant 2 is not necessary, once we\n",
    "\t\t\t# can set eta using the value we desire\n",
    "\t\t\tdE2_dw1 = 2 * Error * -x_i\n",
    "\t\t\tdE2_dtheta = 2 * Error * -1\n",
    "\n",
    "\t\t\t# This is the Gradient Descent method to adapt\n",
    "\t\t\t# weights and theta as defined in the formulation\n",
    "\t\t\tweights = weights - eta * dE2_dw1\n",
    "\t\t\ttheta = theta - eta * dE2_dtheta\n",
    "\n",
    "\t\t\t# Accumulating the squared error to define the stop criterion\n",
    "\t\t\tsumSquaredError = sumSquaredError + Error^2\n",
    "\t\t}\n",
    "\n",
    "\t\tcat(\"Accumulated sum of squared errors = \", sumSquaredError, \"\\n\")\n",
    "\t}\n",
    "\n",
    "\t# Returning weights and theta, once they represent the solution\n",
    "\tret = list()\n",
    "\tret$weights = weights\n",
    "\tret$theta = theta\n",
    "\n",
    "\treturn (ret)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to execute the Perceptron\n",
    "# over unseen data (new examples)\n",
    "perceptron.test <- function(test.table, weights, theta) {\n",
    "\n",
    "\t# Here we print out the expected class (yi) followed by the\n",
    "\t# obtained one (hat_yi) when considering weights and theta.\n",
    "\t# Of course, function perceptron.train should be called\n",
    "\t# previously, in order to find the values for weights and theta\n",
    "\tcat(\"#yi\\that_yi\\n\")\n",
    "\n",
    "\t# Number of input variables\n",
    "\tnVars = ncol(test.table)-1\n",
    "\n",
    "\t# For every row in the test.table\n",
    "\tfor (i in 1:nrow(test.table)) {\n",
    "\n",
    "\t\t# Example i\n",
    "\t\tx_i = test.table[i, 1:nVars]\n",
    "\n",
    "\t\t# Expected class for example i\n",
    "\t\ty_i = test.table[i, ncol(test.table)]\n",
    "\n",
    "\t\t# Output class produced by the Perceptron\n",
    "\t\that_y_i = g(x_i %*% weights + theta)\n",
    "\n",
    "\t\tcat(y_i, \"\\t\", hat_y_i, \"\\n\")\n",
    "\t}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.run.simple <- function() {\n",
    "\n",
    "\t# This is a table with training examples\n",
    "\ttrain.table = matrix(c(0.0, 0,\n",
    "\t\t\t       0.1, 0,\n",
    "\t\t\t       0.2, 0,\n",
    "\t\t\t       0.3, 0,\n",
    "\t\t\t       0.4, 0,\n",
    "\t\t\t       0.5, 0,\n",
    "\t\t\t       0.6, 1,\n",
    "\t\t\t       0.7, 1,\n",
    "\t\t\t       0.8, 1,\n",
    "\t\t\t       0.9, 1,\n",
    "\t\t\t       1.0, 1),\n",
    "\t\t\t      nrow=11,\n",
    "\t\t\t      ncol=2,\n",
    "\t\t\t      byrow=TRUE)\n",
    "\n",
    "\t# This is a table with test examples.\n",
    "\t# The last column only shows the expected\n",
    "\t# output and it is not used in the testing stage\n",
    "\ttest.table = matrix(c(0.05, 0,\n",
    "\t\t\t      0.15, 0,\n",
    "\t\t\t      0.25, 0,\n",
    "\t\t\t      0.35, 0,\n",
    "\t\t\t      0.45, 0,\n",
    "\t\t\t      0.55, 1,\n",
    "\t\t\t      0.65, 1,\n",
    "\t\t\t      0.75, 1,\n",
    "\t\t\t      0.85, 1,\n",
    "\t\t\t      0.95, 1),\n",
    "\t\t\t     nrow=10,\n",
    "\t\t\t     ncol=2,\n",
    "\t\t\t     byrow=TRUE)\n",
    "\n",
    "\t# Training the Perceptron to find weights and theta\n",
    "\ttraining.result = perceptron.train(train.table)\n",
    "\n",
    "\t# Testing the Perceptron with the weights and theta found\n",
    "\tperceptron.test(test.table, training.result$weights, training.result$theta)\n",
    "\n",
    "\treturn (training.result)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the hyperplane found for this simplest\n",
    "# problem which considers a single input variable.\n",
    "# Variables range.start and range.end define the interval of\n",
    "# values for the single input variable composing the problem.\n",
    "# This simple problem has a single variable composing every\n",
    "# example i, which is x_i,1\n",
    "perceptron.simple.hyperplane.plot <- function(weight, theta, range.start=0, range.end=1) {\n",
    "\n",
    "\t# Number of variables is 1\n",
    "\tnVars = 1\n",
    "\n",
    "\t# We will now define the same range for the input variable.\n",
    "\t# This range will contain 100 discretized values\n",
    "\trange_of_every_input_variable = seq(range.start, range.end, length=100)\n",
    "\tx_1 = range_of_every_input_variable\n",
    "\t\n",
    "\t# Computing net for every input value of variable x_i,1\n",
    "\tall_nets = cbind(x_1, 1) %*% c(weight, theta)\n",
    "\n",
    "\t# This variable all_nets contains all net values for all values assumed\n",
    "\t# by variable x_1. Variable hat_y will contain the Perceptron outputs after\n",
    "\t# applying the heaviside function\n",
    "\that_y = rep(0, length(all_nets))\n",
    "\tfor (i in 1:length(all_nets)) {\n",
    "\t\that_y[i] = g(all_nets[i])\n",
    "\t}\n",
    "\n",
    "\t# Variable hyperplane will contain two columns, the first corresponds to\n",
    "\t# the input value of x_i,1 and the second to the class produced by the\n",
    "\t# Perceptron\n",
    "\thyperplane = cbind(x_1, hat_y)\n",
    "\n",
    "\t# Plotting the hyperplane found by the Perceptron\n",
    "\t#plot(hyperplane)\n",
    "   \n",
    "   plot(x_1, hat_y, cex.axis=1.25, cex=1.3, xlab=\"input x_1\", ylab=\"output hat_y (classes) and hyperplane (w,theta)\", cex.lab=1.25, yaxt=\"n\", ylim=c(-0.1,1.1))\n",
    "   lines(hyperplane, lwd=2.3, col=\"gray40\",lty=2)\n",
    "   axis(2, at=c(0,1), labels=c(0,1), cex.axis=1.25,cex.lab=1.25)\n",
    "\n",
    "\treturn (hyperplane)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomizing weights and theta in range [-0.5, 0.5]...\n",
      "Accumulated sum of squared errors =  2 \n",
      "Accumulated sum of squared errors =  2 \n",
      "Accumulated sum of squared errors =  2 \n",
      "Accumulated sum of squared errors =  1 \n",
      "Accumulated sum of squared errors =  0 \n",
      "#yi\that_yi\n",
      "0 \t 0 \n",
      "0 \t 0 \n",
      "0 \t 0 \n",
      "0 \t 0 \n",
      "0 \t 0 \n",
      "1 \t 0 \n",
      "1 \t 1 \n",
      "1 \t 1 \n",
      "1 \t 1 \n",
      "1 \t 1 \n"
     ]
    }
   ],
   "source": [
    "w <- perceptron.run.simple()\n",
    "\n",
    "#perceptron.simple.hyperplane.plot(0.3, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "\n",
       "\n"
      ],
      "text/plain": [
       "named list()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
