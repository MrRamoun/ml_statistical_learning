{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Perceptron Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heaviside function with a default epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g <- function(net, epsilon=0.5) {\n",
    "\tif (net > epsilon) {\n",
    "\t\treturn (1)\n",
    "\t} else {\n",
    "\t\treturn (0)\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Training\n",
    "This is the function that trains the Perceptron model. Observe the ETA and Threshold parameters assume default values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.train <- function(train.table, eta=0.1, threshold=1e-2) {\n",
    "\tnVars = ncol(train.table)-1 # Number of input variables\n",
    "\tcat(\"Randomizing weights and theta in range [-0.5, 0.5]...\\n\")\n",
    "\tweights = runif(min=-0.5, max=0.5, n=nVars) # Randomizing weights\n",
    "\ttheta = runif(min=-0.5, max=0.5, n=1) # Randomizing theta\n",
    "\n",
    "\t# This sum of squared errors will accumulate all errors\n",
    "\t# occuring along training iterations. When this error is\n",
    "\t# below a given threshold, learning stops.\n",
    "\tsumSquaredError = 2*threshold\n",
    "\n",
    "\t# Learning iterations, this will occur until the classifier is\n",
    "    # done with its parameters optimization given a threshold\n",
    "\twhile (sumSquaredError > threshold) {\n",
    "\n",
    "\t\t# Initializing the sum of squared errors as zero\n",
    "\t\t# to start counting and later evaluate the total\n",
    "\t\t# loss for this dataset in train.table\n",
    "\t\tsumSquaredError = 0\n",
    "\n",
    "\t\t# Iterate along all rows (examples) contained in\n",
    "\t\t# train.table\n",
    "\t\tfor (i in 1:nrow(train.table)) {\n",
    "\n",
    "\t\t\t# Example x_i\n",
    "\t\t\tx_i = train.table[i, 1:nVars]\n",
    "\n",
    "\t\t\t# Expected output class\n",
    "\t\t\t# Observe the last column of this table\n",
    "\t\t\t# contains the output class\n",
    "\t\t\ty_i = train.table[i, ncol(train.table)]\n",
    "\n",
    "\t\t\t# Now the Perceptron produces the output\n",
    "\t\t\t# class using the current values for \n",
    "\t\t\t# weights and theta, then it applies the\n",
    "\t\t\t# heaviside function\n",
    "\t\t\that_y_i = g(x_i %*% weights + theta)\n",
    "\n",
    "\t\t\t# This is the error, referred to as (y_i - g(x_i))\n",
    "\t\t\t# in the Perceptron formulation\n",
    "\t\t\tError = y_i - hat_y_i\n",
    "\n",
    "\t\t\t# As part of the Gradient Descent method, we here\n",
    "\t\t\t# compute the partial derivative of the Squared Error\n",
    "\t\t\t# for the current example i in terms of weights and theta.\n",
    "\t\t\t# Observe constant 2 is not necessary, once we\n",
    "\t\t\t# can set eta using the value we desire\n",
    "\t\t\tdE2_dw1 = 2 * Error * -x_i\n",
    "\t\t\tdE2_dtheta = 2 * Error * -1\n",
    "\n",
    "\t\t\t# This is the Gradient Descent method to adapt\n",
    "\t\t\t# weights and theta as defined in the formulation\n",
    "\t\t\tweights = weights - eta * dE2_dw1\n",
    "\t\t\ttheta = theta - eta * dE2_dtheta\n",
    "\n",
    "\t\t\t# Accumulating the squared error to define the stop criterion\n",
    "\t\t\tsumSquaredError = sumSquaredError + Error^2\n",
    "\t\t}\n",
    "\n",
    "\t\tcat(\"Accumulated sum of squared errors = \", sumSquaredError, \"\\n\")\n",
    "\t}\n",
    "\n",
    "\t# Returning weights and theta, once they represent the solution\n",
    "\tret = list()\n",
    "\tret$weights = weights\n",
    "\tret$theta = theta\n",
    "\n",
    "\treturn (ret)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the function to execute the Perceptron over unseen data (new examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.test <- function(test.table, weights, theta) {\n",
    "\n",
    "\t# Here we print out the expected class (yi) followed by the\n",
    "\t# obtained one (hat_yi) when considering weights and theta.\n",
    "\t# Of course, function perceptron.train should be called\n",
    "\t# previously, in order to find the values for weights and theta\n",
    "\tcat(\"#yi\\that_yi\\n\")\n",
    "\n",
    "\t# Number of input variables\n",
    "\tnVars = ncol(test.table)-1\n",
    "\n",
    "\t# For every row in the test.table\n",
    "\tfor (i in 1:nrow(test.table)) {\n",
    "\n",
    "\t\t# Example i\n",
    "\t\tx_i = test.table[i, 1:nVars]\n",
    "\n",
    "\t\t# Expected class for example i\n",
    "\t\ty_i = test.table[i, ncol(test.table)]\n",
    "\n",
    "\t\t# Output class produced by the Perceptron\n",
    "\t\that_y_i = g(x_i %*% weights + theta)\n",
    "\n",
    "\t\tcat(y_i, \"\\t\", hat_y_i, \"\\n\")\n",
    "\t}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.run.simple <- function() {\n",
    "\t# This is a table with training examples\n",
    "\ttrain.table = matrix(c(0.0, 0,\n",
    "\t\t\t       0.1, 0,\n",
    "\t\t\t       0.2, 0,\n",
    "\t\t\t       0.3, 0,\n",
    "\t\t\t       0.4, 0,\n",
    "\t\t\t       0.5, 0,\n",
    "\t\t\t       0.6, 1,\n",
    "\t\t\t       0.7, 1,\n",
    "\t\t\t       0.8, 1,\n",
    "\t\t\t       0.9, 1,\n",
    "\t\t\t       1.0, 1),\n",
    "\t\t\t      nrow=11,\n",
    "\t\t\t      ncol=2,\n",
    "\t\t\t      byrow=TRUE)\n",
    "\t# This is a table with test examples.\n",
    "\t# The last column only shows the expected\n",
    "\t# output and it is not used in the testing stage\n",
    "\ttest.table = matrix(c(0.05, 0,\n",
    "\t\t\t      0.15, 0,\n",
    "\t\t\t      0.25, 0,\n",
    "\t\t\t      0.35, 0,\n",
    "\t\t\t      0.45, 0,\n",
    "\t\t\t      0.55, 1,\n",
    "\t\t\t      0.65, 1,\n",
    "\t\t\t      0.75, 1,\n",
    "\t\t\t      0.85, 1,\n",
    "\t\t\t      0.95, 1),\n",
    "\t\t\t     nrow=10,\n",
    "\t\t\t     ncol=2,\n",
    "\t\t\t     byrow=TRUE)\n",
    "\n",
    "\t# Training the Perceptron to find weights and theta\n",
    "\ttraining.result = perceptron.train(train.table)\n",
    "\n",
    "\t# Testing the Perceptron with the weights and theta found\n",
    "\tperceptron.test(test.table, training.result$weights, training.result$theta)\n",
    "\n",
    "\treturn (training.result)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots the hyperplane found for this simplest problem which considers a single input variable. Variables range.start and range.end define the interval of values for the single input variable composing the problem. This simple problem has a single variable composing every example i, which is x_i,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.simple.hyperplane.plot <- function(weight, theta, range.start=0, range.end=1) {\n",
    "\n",
    "\t# Number of variables is 1\n",
    "\tnVars = 1\n",
    "\n",
    "\t# We will now define the same range for the input variable.\n",
    "\t# This range will contain 100 discretized values\n",
    "\trange_of_every_input_variable = seq(range.start, range.end, length=100)\n",
    "\tx_1 = range_of_every_input_variable\n",
    "\t\n",
    "\t# Computing net for every input value of variable x_i,1\n",
    "\tall_nets = cbind(x_1, 1) %*% c(weight, theta)\n",
    "\n",
    "\t# This variable all_nets contains all net values for all values assumed\n",
    "\t# by variable x_1. Variable hat_y will contain the Perceptron outputs after\n",
    "\t# applying the heaviside function\n",
    "\that_y = rep(0, length(all_nets))\n",
    "\tfor (i in 1:length(all_nets)) {\n",
    "\t\that_y[i] = g(all_nets[i])\n",
    "\t}\n",
    "\n",
    "\t# Variable hyperplane will contain two columns, the first corresponds to\n",
    "\t# the input value of x_i,1 and the second to the class produced by the\n",
    "\t# Perceptron\n",
    "\thyperplane = cbind(x_1, hat_y)\n",
    "\n",
    "\t# Plotting the hyperplane found by the Perceptron\n",
    "\t#plot(hyperplane)\n",
    "   \n",
    "   plot(x_1, hat_y, cex.axis=1.25, cex=1.3, xlab=\"input x_1\", ylab=\"output hat_y (classes) and hyperplane (w,theta)\", cex.lab=1.25, yaxt=\"n\", ylim=c(-0.1,1.1))\n",
    "   lines(hyperplane, lwd=2.3, col=\"gray40\",lty=2)\n",
    "   axis(2, at=c(0,1), labels=c(0,1), cex.axis=1.25,cex.lab=1.25)\n",
    "\n",
    "\treturn (hyperplane)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperplane <- perceptron.run.simple()\n",
    "\n",
    "perceptron.simple.hyperplane.plot(hyperplane$weights, hyperplane$theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Hyperplane without G\n",
    "\n",
    "This function plots the hyperplane found for this simplest problem which considers a single input variable and  function net only. Variables range.start and range.end define the interval of values for the single input variable composing the problem. This simple problem has a single variable composing every example i, which is x_i,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.simple.hyperplane.plot.without.g <- function(weight, theta, range.start=0, range.end=1) {\n",
    "\n",
    "\t# Number of variables is 1\n",
    "\tnVars = 1\n",
    "\n",
    "\t# We will now define the same range for the input variable.\n",
    "\t# This range will contain 100 discretized values\n",
    "\trange_of_every_input_variable = seq(range.start, range.end, length=100)\n",
    "\tx_1 = range_of_every_input_variable\n",
    "\t\n",
    "\t# Computing net for every input value of variable x_i,1\n",
    "\tall_nets = cbind(x_1, 1) %*% c(weight, theta)\n",
    "\n",
    "\t# This variable all_nets contains all net values for every value assumed\n",
    "\t# by variable x_1. Variable hat_y will contain the Perceptron outputs before\n",
    "\t# applying the heaviside function\n",
    "\that_y = rep(0, length(all_nets))\n",
    "\tfor (i in 1:length(all_nets)) {\n",
    "\t\that_y[i] = all_nets[i] # Without the heaviside function g(net)\n",
    "\t}\n",
    "\n",
    "\t# Variable hyperplane will contain two columns, the first corresponds to\n",
    "\t# the input value of x_i,1 and the second to the class produced by the\n",
    "\t# Perceptron\n",
    "\thyperplane = cbind(x_1, hat_y)\n",
    "\n",
    "\t# Plotting the hyperplane found by the Perceptron in terms of function net\n",
    "\tplot(hyperplane)\n",
    "\n",
    "\treturn (hyperplane)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Perceptron Hyperplane without G\n",
    "\n",
    "This function is used to run the training stage for the simplest classification task. Each training will produce a different pair of weight and theta, which are then used to plot function net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run.several.times <- function(times=5) {\n",
    "\n",
    "\t# Saving the results for each function net\n",
    "\tnet.functions = list()\n",
    "\n",
    "\t# For each execution\n",
    "\tfor (i in 1:times) {\n",
    "\t\t# Call training\n",
    "\t\ttraining.execution = perceptron.run.simple()\n",
    "\n",
    "\t\t# Obtaining function net\n",
    "\t\tnet.functions[[i]] = perceptron.simple.hyperplane.plot.without.g(\n",
    "\t\t\t\t\ttraining.execution$weight, \n",
    "\t\t\t\t\ttraining.execution$theta)\n",
    "\t}\n",
    "\n",
    "\t# Plotting\n",
    "\tplot(net.functions[[1]], col=1)\n",
    "\tfor (i in 2:times) {\n",
    "\t\tpoints(net.functions[[i]], col=i)\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.run.AND <- function() {\n",
    "\n",
    "\t# This is a table with all possible examples\n",
    "\t# for the problem AND. In this case we will\n",
    "\t# use the same table for training and testing,\n",
    "\t# just because we know all possible binary\n",
    "\t# combinations\n",
    "\ttable = matrix(c(0, 0, 0,  # 0 AND 0 = 0\n",
    "\t\t\t 0, 1, 0,  # 0 AND 1 = 0\n",
    "\t\t\t 1, 0, 0,  # 1 AND 0 = 0\n",
    "\t\t\t 1, 1, 1), # 1 AND 1 = 1\n",
    "\t\t\tnrow=4,\n",
    "\t\t\tncol=3,\n",
    "\t\t\tbyrow=TRUE)\n",
    "\n",
    "\t# Training the Perceptron to find weights and theta\n",
    "\ttraining.result = perceptron.train(table)\n",
    "\n",
    "\t# Testing the Perceptron with the weights and theta found\n",
    "\tperceptron.test(table, training.result$weights, training.result$theta)\n",
    "\n",
    "\t# Plotting the hyperplane found\n",
    "\tperceptron.hyperplane.plot(training.result$weights, training.result$theta)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots the hyperplane found for a given classification task with two input variables only. Variables range.start and range.end define the interval of values for every one of those input variables composing the problem. The problem AND has two variables composing each example i, which are x_i,1 and x_i,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.hyperplane.plot <- function(weights, theta, range.start=0, range.end=1) {\n",
    "\n",
    "\t# Variable weights define the number of input variables we have,\n",
    "\t# so we can use this information to create axes in a multidimensional\n",
    "\t# input space in order to see how inputs modify the output class\n",
    "\t# provided by the Perceptron\n",
    "\tnVars = length(weights)\n",
    "\n",
    "\t# We will now define the same range for every input variable.\n",
    "\t# This range will contain 100 discretized values\n",
    "\trange_of_every_input_variable = seq(range.start, range.end, length=100)\n",
    "\n",
    "\tx_1 = range_of_every_input_variable\n",
    "\tx_2 = range_of_every_input_variable\n",
    "\t\n",
    "\t# Function outer combines every possible value for variable x_1\n",
    "\t# against every possible value for x_2. Observe they are continuous values\n",
    "\t# which were never seen (we expect either 0 or 1) by this Perceptron during \n",
    "\t# the training stage. Also observe value 1 inside the cbind, which refers\n",
    "\t# to the 1 * theta while computing function net. Operation %*% corresponds\n",
    "\t# to the dot product.\n",
    "\tall_nets = outer(x_1, x_2, function(x, y) { cbind(x, y, 1) %*% c(weights, theta) } )\n",
    "\n",
    "\t# This variable all_nets contains all net values for every combination between\n",
    "\t# variables x_1 and x_2. Variable y will contain the Perceptron outputs after\n",
    "\t# applying the heaviside function\n",
    "\ty = matrix(0, nrow=nrow(all_nets), ncol=ncol(all_nets))\n",
    "\tfor (row in 1:nrow(all_nets)) {\n",
    "\t\tfor (col in 1:ncol(all_nets)) {\n",
    "\t\t\ty[row, col] = g(all_nets[row, col])\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\t# Plotting the hyperplane found by the Perceptron\n",
    "\tfilled.contour(x_1, x_2, y)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Perceptron's error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.simple.error <- function(range.start=-1, range.end=1, mu=1e-10) {\n",
    "\n",
    "\t# Defining the table with examples\n",
    "\ttable = cbind(seq(0, 0.5, length=100), rep(0, 100)) # negative examples\n",
    "\ttable = rbind(table, cbind(seq(0.5+mu, 1, length=100), rep(1, 100))) # positive examples\n",
    "\n",
    "\t# We will now define the same range for the free variables weight and theta.\n",
    "\t# This range will contain 100 discretized values\n",
    "\trange_for_free_variables = seq(range.start, range.end, length=100)\n",
    "\tweight = range_for_free_variables\n",
    "\ttheta = range_for_free_variables\n",
    "\t\n",
    "\t# Sum of errors while varying weight and theta\n",
    "\terror_function = matrix(0, nrow=length(weight), ncol=length(theta))\n",
    "\n",
    "\t# For each weight\n",
    "\tfor (w in 1:length(weight)) {\n",
    "\t\t# For each theta\n",
    "\t\tfor (t in 1:length(theta)) {\n",
    "\t\t\t# Compute all net values\n",
    "\t\t\tnet = cbind(table[,1], rep(1, nrow(table))) %*% c(weight[w], theta[t])\n",
    "\t\t\t# Defining a vector to save the Perceptron outputs\n",
    "\t\t\that_y = rep(0, length(net))\n",
    "\t\t\t# Producing the output classes\n",
    "\t\t\tfor (n in 1:length(net)) {\n",
    "\t\t\t\t# We removed function g(net) to improve illustration\n",
    "\t\t\t\that_y[n] = net[n] \n",
    "\t\t\t}\n",
    "\n",
    "\t\t\t# These are the expected classes\n",
    "\t\t\ty = table[,2]\n",
    "\t\t\t# Expected minus the obtained classes to provide the error\n",
    "\t\t\terror = y - hat_y\n",
    "\t\t\t# Saving the total error in the matrix\n",
    "\t\t\terror_function[w, t] = sum(error) # This brings up amortization problems\n",
    "\t\t\t\t\t\t\t  # because positive and negative terms\n",
    "\t\t\t\t\t\t\t  # will cancel each other\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\t# Plotting the error\n",
    "\tfilled.contour(error_function)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron.simple.squared.error <- function(range.start=-10, range.end=10, mu=1e-10) {\n",
    "\n",
    "\t# Defining the table with examples\n",
    "\ttable = cbind(seq(0, 0.5, length=100), rep(0, 100))\n",
    "\ttable = rbind(table, cbind(seq(0.5+mu, 1, length=100), rep(1, 100)))\n",
    "\n",
    "\t# We will now define the same range for the free variables weight and theta.\n",
    "\t# This range will contain 100 discretized values\n",
    "\trange_for_free_variables = seq(range.start, range.end, length=50)\n",
    "\tweight = range_for_free_variables\n",
    "\ttheta = range_for_free_variables\n",
    "\t\n",
    "\t# Sum of squared errors while varying weight and theta\n",
    "\terror_function = matrix(0, nrow=length(weight), ncol=length(theta))\n",
    "\n",
    "\t# For each weight\n",
    "\tfor (w in 1:length(weight)) {\n",
    "\t\t# For each theta\n",
    "\t\tfor (t in 1:length(theta)) {\n",
    "\t\t\t# Compute all net values\n",
    "\t\t\tnet = cbind(table[,1], rep(1, nrow(table))) %*% c(weight[w], theta[t])\n",
    "\t\t\t# Defining a vector to save the Perceptron outputs\n",
    "\t\t\that_y = rep(0, length(net))\n",
    "\t\t\t# Producing the output classes\n",
    "\t\t\tfor (n in 1:length(net)) {\n",
    "\t\t\t\t# We removed function g(net) to improve illustration\n",
    "\t\t\t\that_y[n] = net[n]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\t# These are the expected classes\n",
    "\t\t\ty = table[,2]\n",
    "\t\t\t# Expected minus the obtained classes to provide the error,\n",
    "\t\t\t# which is then squared to avoid negative and positive values\n",
    "\t\t\t# that amortize each other\n",
    "\t\t\tsquared.error = (y - hat_y)^2\n",
    "\t\t\t# Saving the total squared error in the matrix\n",
    "\t\t\terror_function[w, t] = sum(squared.error)\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\t# We apply a log on the squared error function to improve illustration,\n",
    "\t# otherwise we do not see the paraboloid as clear as in this form.\n",
    "\tfilled.contour(log(error_function))\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
